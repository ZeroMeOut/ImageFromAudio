## Audio to Image using various models
Recently I have been interested in audio-to-image generation. That is using the audio of something, let's say the sound of being inside a beach to generate an image of a beach. Now I didn't find a dataset that pairs audio to images, so I made one based on [this paper](https://arxiv.org/pdf/2109.13354.pdf). Basically an MNSIT-FSDD dataset. I also made branches for each step of the process while I was trying to figure out things. First was WGAN, I used a [YouTube tutorial](https://youtu.be/pG0QZ7OddX4?si=brUfAghFf2_xbcc6) for that. Then I went on to learn about and code up a VAE (specifically a CVAE) with conv layers. I also coded up a VAEGAN and used the loss from the first paper I linked (I am not still sure it's correct lol). I would show their test results later, I wanna experiment on one more before I am done with this :)
